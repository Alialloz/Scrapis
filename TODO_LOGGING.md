# TODO : Int√©gration compl√®te du logging

##  Statut actuel

‚úÖ **Syst√®me de logging cr√©√©** (`logger_config.py`)
‚úÖ **Script d'analyse cr√©√©** (`analyze_logs.py`)  
‚úÖ **Documentation cr√©√©e** (`LOGGING.md`)
üîÑ **Int√©gration partielle** dans `scraper_production.py`
‚ùå **Non int√©gr√©** dans les autres fichiers

## ‚úÖ D√©j√† fait

- `scraper_production.py` : 
  - Logger configur√© en haut du fichier
  - `send_to_api()` : Migr√© vers logging
  - R√©sum√© du cycle : Utilise `log_scraping_stats()`
  - Import des erreurs config : Migr√© vers logging

## ‚ùå √Ä faire : scraper_production.py

Remplacer les `print()` restants (environ 60) par des appels au logger :

### Priorit√© HAUTE

- `run_monitoring_cycle()` : 
  - Ligne 124-126 : En-t√™te du cycle
  - Ligne 150 : Info limite annonces
  - Ligne 155 : Traitement annonce
  - Ligne 168-180 : Sauvegarde et erreurs
  - Ligne 190 : Pause
  - Ligne 210-212 : Erreur critique du cycle

- `start_monitoring()` :
  - Ligne 377-385 : En-t√™te de d√©marrage
  - Ligne 392 : Num√©ro de cycle
  - Ligne 405-407 : Fin de cycle
  - Ligne 413-416 : Arr√™t monitoring

- `main()` :
  - Ligne 423-447 : V√©rification configuration
  - Ligne 453-455 : Lancement monitoring
  - Ligne 460-464 : Erreur critique

### Priorit√© MOYENNE

- `cleanup_json_files()` : Tous les print() (ligne 293-367)
- `backup_scraped_ids()` : Tous les print() (ligne 257-270)
- `save_stats()` : Ligne 239

### Guide de migration

```python
# Remplacements √† faire:

print("[ERREUR]...")      ‚Üí logger.error("...")
print("[WARNING]...")     ‚Üí logger.warning("...")
print("[INFO]...")        ‚Üí logger.info("...")
print("[OK]...")          ‚Üí logger.info("‚úì ...")
print(f"[ERREUR]...{var}") ‚Üí logger.error(f"...{var}")

# Pour les traceback:
except Exception as e:
    print(f"[ERREUR] {e}")
    traceback.print_exc()

# Devient:
except Exception as e:
    logger.error(f"Erreur: {e}", exc_info=True)
```

## ‚ùå √Ä faire : scraper_monitor.py

Ce fichier est utilis√© par `scraper_production.py`, il DOIT aussi avoir du logging :

1. Ajouter en haut :
```python
from logger_config import setup_logger, log_extraction_result

logger = setup_logger('monitor', level='INFO')
```

2. Remplacer tous les `print()` dans :
   - `__init__()` 
   - `get_new_listings()`
   - `scrape_listing()`
   - `run_once()`

## ‚ùå √Ä faire : scraper_detail_complete.py

Moins urgent, mais am√©liorerait le debug :

- Garder les `print()` actuels OU
- Ajouter du logging en parall√®le pour le d√©veloppement

## üß™ Tests

Une fois l'int√©gration termin√©e :

```bash
# Test local
python tests/test_logging.py

# Lancer le scraper en production (test)
python scraper_production.py

# Analyser les logs
python analyze_logs.py

# V√©rifier les logs
tail -f logs/production.log
tail -f logs/monitor.log
```

## üìù Checklist finale

- [ ] `scraper_production.py` : 100% migr√©
- [ ] `scraper_monitor.py` : 100% migr√©
- [ ] Tests locaux passent
- [ ] Logs cr√©√©s dans `logs/`
- [ ] `analyze_logs.py` fonctionne
- [ ] Commit + push
- [ ] D√©ployer sur droplet
- [ ] V√©rifier logs sur serveur

## üöÄ D√©ploiement

```bash
git add -A
git commit -m "Feat: Int√©gration logging dans scraper_production"
git push origin main

# Sur le serveur
ssh root@IP
cd /opt/scraper-centris
git pull
systemctl restart scraper-centris

# V√©rifier les logs
tail -f logs/production.log
python analyze_logs.py
```
