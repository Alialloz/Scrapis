# üöÄ GUIDE DE D√âPLOIEMENT - SCRAPER CENTRIS

## üìã Pr√©-requis

- ‚úÖ Python 3.8 ou sup√©rieur install√©
- ‚úÖ Toutes les d√©pendances install√©es (`pip install -r requirements.txt`)
- ‚úÖ ChromeDriver compatible avec votre version de Chrome
- ‚úÖ URL de votre API pr√™te

---

## üîß √âTAPE 1 : Configuration de l'API

### 1.1 √âditer `config_api.py`

Ouvrez le fichier `config_api.py` et configurez :

```python
# URL de votre API
API_ENDPOINT = "https://votre-api.com/api/properties"  # ‚Üê MODIFIER ICI

# Headers (si authentification requise)
API_HEADERS = {
    'Content-Type': 'application/json',
    'Authorization': 'Bearer VOTRE_TOKEN_ICI',  # ‚Üê D√©commenter et configurer
}
```

### 1.2 Tester la connexion √† l'API

```bash
python test_api.py
```

Ce script va :
- V√©rifier que l'API est accessible
- Envoyer un JSON de test
- Afficher la r√©ponse

---

## üß™ √âTAPE 2 : Test avec une annonce

Avant de lancer le monitoring continu, testez avec une seule annonce :

```bash
python scraper_with_list_info.py
```

V√©rifiez que :
- ‚úÖ Le scraping fonctionne
- ‚úÖ Les 9 photos sont extraites
- ‚úÖ La source est correcte (pas "s.")
- ‚úÖ Le JSON est complet

---

## ‚ñ∂Ô∏è √âTAPE 3 : Lancement du Monitoring Continu

### Windows

Double-cliquez sur `start_monitoring.bat` ou :

```cmd
python scraper_production.py
```

### Linux/Mac

```bash
chmod +x start_monitoring.sh
./start_monitoring.sh
```

ou :

```bash
python3 scraper_production.py
```

---

## üîÑ Fonctionnement du Monitoring

### Cycle Automatique

Le syst√®me va :
1. **Toutes les heures** : Scanner la page Matrix
2. **D√©tecter** les nouvelles annonces (via num√©ro Centris)
3. **Scraper** chaque nouvelle annonce (60 sec/annonce)
4. **Sauvegarder** localement dans `property_XXXXXXXX.json`
5. **Envoyer** le JSON complet √† votre API
6. **Enregistrer** l'ID dans `scraped_properties.json`
7. **Attendre** 1 heure avant le prochain cycle

### Logs

Le syst√®me affiche en temps r√©el :
- Nombre d'annonces trouv√©es
- Nouvelles annonces d√©tect√©es
- Progression du scraping
- Statut de l'envoi √† l'API
- R√©sum√© du cycle

---

## üìä Fichiers G√©n√©r√©s

| Fichier | Description |
|---------|-------------|
| `scraped_properties.json` | Liste des IDs d√©j√† scrap√©s avec dates |
| `property_XXXXXXXX.json` | Donn√©es compl√®tes de chaque propri√©t√© |
| `monitoring_stats.json` | Statistiques des 100 derniers cycles |

---

## ‚öôÔ∏è Configuration Avanc√©e

### Modifier l'intervalle de monitoring

Dans `config_api.py` :

```python
MONITORING_INTERVAL = 30  # Minutes (30 = toutes les 30 minutes)
```

### D√©sactiver la sauvegarde locale

```python
SAVE_JSON_LOCALLY = False  # Ne pas sauvegarder les JSON localement
```

### Limiter le nombre d'annonces par cycle

```python
MAX_LISTINGS_PER_CYCLE = 5  # Scraper max 5 annonces par cycle
```

---

## üñ•Ô∏è D√âPLOIEMENT EN PRODUCTION

### Option 1 : Service Windows

1. Cr√©er une t√¢che planifi√©e Windows :
   - Ouvrir "Planificateur de t√¢ches"
   - Cr√©er une t√¢che de base
   - Action : `python.exe C:\chemin\vers\scraper_production.py`
   - D√©clencheur : Au d√©marrage du syst√®me
   - Options : Red√©marrer en cas d'√©chec

### Option 2 : Service Linux (systemd)

Cr√©er `/etc/systemd/system/centris-scraper.service` :

```ini
[Unit]
Description=Centris Scraper Monitoring Service
After=network.target

[Service]
Type=simple
User=votre-utilisateur
WorkingDirectory=/chemin/vers/Scrapis
ExecStart=/usr/bin/python3 scraper_production.py
Restart=always
RestartSec=60

[Install]
WantedBy=multi-user.target
```

Activer et d√©marrer :
```bash
sudo systemctl enable centris-scraper
sudo systemctl start centris-scraper
sudo systemctl status centris-scraper
```

### Option 3 : Cron (Linux/Mac)

Pour un monitoring toutes les heures :

```bash
crontab -e
```

Ajouter :
```cron
0 * * * * cd /chemin/vers/Scrapis && python3 scraper_production.py --single-cycle
```

### Option 4 : Docker

Cr√©er `Dockerfile` :

```dockerfile
FROM python:3.9-slim

# Installer Chrome
RUN apt-get update && apt-get install -y \
    wget \
    gnupg \
    chromium \
    chromium-driver

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD ["python", "scraper_production.py"]
```

Build et run :
```bash
docker build -t centris-scraper .
docker run -d --restart always centris-scraper
```

---

## üîç Monitoring et Maintenance

### V√©rifier que le service tourne

```bash
# Linux
ps aux | grep scraper_production

# Windows
tasklist | findstr python
```

### Consulter les logs en temps r√©el

Le script affiche tout dans la console. Pour rediriger vers un fichier :

```bash
python scraper_production.py > logs/scraper.log 2>&1
```

### Consulter les statistiques

```bash
cat monitoring_stats.json
```

### R√©initialiser le syst√®me

Pour tout rescraper depuis le d√©but :

```bash
rm scraped_properties.json
```

---

## üêõ D√©pannage

### Probl√®me : Le monitoring ne d√©marre pas

**V√©rifications :**
- Python install√© ? `python --version`
- D√©pendances install√©es ? `pip install -r requirements.txt`
- ChromeDriver install√© ? Le script l'installe automatiquement

### Probl√®me : L'API ne re√ßoit pas les donn√©es

**V√©rifications :**
1. L'API_ENDPOINT est-il correct dans `config_api.py` ?
2. L'API est-elle accessible ? `curl https://votre-api.com/api/properties`
3. Les logs montrent-ils des erreurs ?
4. Testez avec `test_api.py`

### Probl√®me : Erreur "No such element"

**Solution :**
- La page Matrix a peut-√™tre chang√© de structure
- Augmentez les d√©lais d'attente dans le code
- V√©rifiez que la page se charge correctement

### Probl√®me : ChromeDriver incompatible

**Solution :**
```bash
pip install --upgrade selenium webdriver-manager
```

### Probl√®me : M√©moire insuffisante

**Solution :**
- Limiter le nombre d'annonces par cycle dans `config_api.py`
- Fermer Chrome entre chaque scraping (d√©j√† fait)

---

## üìû Support

### Logs importants √† fournir :

1. Sortie console compl√®te
2. Contenu de `scraped_properties.json`
3. Un fichier `property_XXXXXXXX.json` exemple
4. Version de Python : `python --version`
5. Version de Chrome

---

## üéØ Checklist de D√©ploiement

Avant de mettre en production :

- [ ] API configur√©e dans `config_api.py`
- [ ] Test r√©ussi avec `scraper_with_list_info.py`
- [ ] Test API r√©ussi avec `test_api.py`
- [ ] Intervalle de monitoring configur√© (60 minutes)
- [ ] Service/t√¢che planifi√©e configur√©
- [ ] Logs redirig√©s vers un fichier
- [ ] M√©canisme de red√©marrage automatique en place
- [ ] Monitoring des erreurs en place
- [ ] Espace disque suffisant pour les JSON

---

## üìà Estimation des Ressources

### Espace Disque

- ~10 KB par annonce (JSON)
- Si 100 nouvelles annonces/mois = 1 MB/mois
- Photos non t√©l√©charg√©es (seulement URLs)

### M√©moire

- ~200-300 MB pendant le scraping
- ~50 MB au repos

### CPU

- Pics √† 50% pendant le scraping
- ~0% au repos

### Bande Passante

- ~1 MB par annonce scrap√©e
- ~60 MB pour scraper 60 annonces

---

## ‚úÖ Le Syst√®me Est Pr√™t !

Une fois configur√©, le syst√®me tournera **automatiquement 24/7** et :
- ‚úÖ D√©tectera les nouvelles annonces toutes les heures
- ‚úÖ Scrapera automatiquement toutes les donn√©es
- ‚úÖ Enverra le JSON complet √† votre API
- ‚úÖ Ne re-scrapera jamais une annonce d√©j√† trait√©e

**Le d√©ploiement est simple et robuste ! üöÄ**

