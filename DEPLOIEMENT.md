# ðŸš€ GUIDE DE DÃ‰PLOIEMENT - Scraper Centris sur DigitalOcean

## ðŸ“‹ PrÃ©requis

Vous devez avoir :
- âœ… Un Droplet DigitalOcean crÃ©Ã© (Ubuntu 22.04/24.04)
- âœ… L'adresse IP de votre Droplet
- âœ… AccÃ¨s SSH (clÃ© SSH ou mot de passe root)
- âœ… Votre code en local

---

## ðŸŽ¯ DÃ‰PLOIEMENT EN 5 Ã‰TAPES

### **Ã‰TAPE 1 : Connexion au serveur**

```bash
# Remplacez VOTRE_IP par l'IP de votre Droplet
ssh root@VOTRE_IP
```

**Si premiÃ¨re connexion :**
```
Are you sure you want to continue connecting (yes/no)? 
# Tapez: yes
```

---

### **Ã‰TAPE 2 : TÃ©lÃ©charger le script d'installation**

```bash
# Sur le serveur
cd /root
wget https://raw.githubusercontent.com/VOTRE-REPO/Scrapis/main/install_server.sh
chmod +x install_server.sh
```

**OU** si vous n'avez pas Git/GitHub, copiez le contenu du fichier `install_server.sh` :

```bash
# Sur le serveur
nano install_server.sh
# Collez le contenu du fichier install_server.sh
# Ctrl+O pour sauvegarder, Ctrl+X pour quitter
chmod +x install_server.sh
```

---

### **Ã‰TAPE 3 : ExÃ©cuter l'installation (10-15 minutes)**

```bash
sudo ./install_server.sh
```

**Ce script installe automatiquement :**
- âœ… Python 3.12
- âœ… Google Chrome
- âœ… ChromeDriver
- âœ… Toutes les dÃ©pendances systÃ¨me
- âœ… Service systemd
- âœ… Configuration des logs

â˜• **Prenez un cafÃ©, Ã§a prend ~10 minutes...**

---

### **Ã‰TAPE 4 : DÃ©ployer votre code**

#### **Option A : Avec Git (recommandÃ©)**

```bash
cd /opt/scraper-centris
git clone https://github.com/VOTRE-REPO/Scrapis.git .
```

#### **Option B : Upload manuel depuis votre PC**

```bash
# Sur VOTRE PC (pas le serveur)
cd /chemin/vers/Scrapis
scp -r *.py *.txt root@VOTRE_IP:/opt/scraper-centris/
```

**VÃ©rifiez que les fichiers sont lÃ  :**
```bash
# Sur le serveur
ls -la /opt/scraper-centris/
# Vous devriez voir: scraper_production.py, config_api.py, etc.
```

---

### **Ã‰TAPE 5 : Installer les dÃ©pendances Python**

```bash
cd /opt/scraper-centris
pip3.12 install -r requirements.txt
```

**Si requirements.txt n'existe pas :**
```bash
pip3.12 install selenium beautifulsoup4 requests webdriver-manager pandas
```

---

## âš™ï¸ CONFIGURATION

### **VÃ©rifier config_api.py**

```bash
nano /opt/scraper-centris/config_api.py
```

**VÃ©rifiez que l'URL API est correcte :**
```python
API_ENDPOINT = "https://api.rayharvey.ca/robot/api/scraping"  # âœ“ Bon
```

`Ctrl+X` pour quitter (pas besoin de modifier si dÃ©jÃ  correct)

---

## ðŸš€ LANCEMENT DU SERVICE

### **DÃ©marrer le scraper**

```bash
systemctl start scraper-centris
```

### **Activer le dÃ©marrage automatique**

```bash
systemctl enable scraper-centris
```

### **VÃ©rifier que Ã§a tourne**

```bash
systemctl status scraper-centris
```

**RÃ©sultat attendu :**
```
â— scraper-centris.service - Scraper Centris - Monitoring automatique
   Active: active (running) since...
```

---

## ðŸ“Š SURVEILLANCE

### **Voir les logs en temps rÃ©el**

```bash
tail -f /var/log/scraper-centris.log
```

`Ctrl+C` pour quitter

### **Voir les erreurs**

```bash
tail -f /var/log/scraper-centris-error.log
```

### **DerniÃ¨res 50 lignes**

```bash
tail -50 /var/log/scraper-centris.log
```

---

## ðŸ”§ COMMANDES UTILES

### **RedÃ©marrer le service**
```bash
systemctl restart scraper-centris
```

### **ArrÃªter le service**
```bash
systemctl stop scraper-centris
```

### **Voir le statut**
```bash
systemctl status scraper-centris
```

### **DÃ©sactiver le dÃ©marrage automatique**
```bash
systemctl disable scraper-centris
```

### **Voir les logs avec filtres**
```bash
# Seulement les erreurs
grep -i error /var/log/scraper-centris.log

# Seulement les succÃ¨s API
grep -i "API.*succes" /var/log/scraper-centris.log

# Statistiques des cycles
grep -i "RESUME DU CYCLE" /var/log/scraper-centris.log -A 6
```

---

## ðŸ”„ MISE Ã€ JOUR DU CODE

### **MÃ©thode 1 : Avec Git**

```bash
cd /opt/scraper-centris
git pull
systemctl restart scraper-centris
```

### **MÃ©thode 2 : Avec le script deploy.sh**

**Sur VOTRE PC (pas le serveur) :**

1. Ã‰ditez `deploy.sh` :
```bash
nano deploy.sh
# Changez SERVER_IP="VOTRE_IP_SERVEUR" avec la vraie IP
```

2. ExÃ©cutez :
```bash
chmod +x deploy.sh
./deploy.sh
```

Le script :
- âœ… CrÃ©e une archive du code
- âœ… L'envoie sur le serveur
- âœ… La dÃ©compresse
- âœ… RedÃ©marre le service

---

## ðŸ› DÃ‰PANNAGE

### **Le service ne dÃ©marre pas**

```bash
# Voir les logs dÃ©taillÃ©s
journalctl -u scraper-centris -n 50

# Tester manuellement
cd /opt/scraper-centris
python3.12 scraper_production.py
```

### **Erreur "Chrome not found"**

```bash
# VÃ©rifier Chrome
google-chrome --version

# RÃ©installer si besoin
sudo ./install_server.sh
```

### **Erreur "Permission denied"**

```bash
# Donner les permissions
chmod +x /opt/scraper-centris/*.py
chmod +x /opt/scraper-centris/*.sh
```

### **Le scraper ne trouve pas les annonces**

```bash
# VÃ©rifier la connexion
curl https://api.rayharvey.ca/robot/api/scraping

# Tester manuellement
cd /opt/scraper-centris
python3.12 test_api.py
```

### **Espace disque plein**

```bash
# Voir l'espace disque
df -h

# Nettoyer les fichiers JSON anciens
cd /opt/scraper-centris
rm property_*.json  # Garde scraped_properties.json !

# Nettoyer les logs (attention !)
> /var/log/scraper-centris.log
```

---

## ðŸ“ˆ MONITORING & MAINTENANCE

### **VÃ©rification quotidienne**

```bash
# Statut rapide
systemctl status scraper-centris

# DerniÃ¨re activitÃ©
tail -20 /var/log/scraper-centris.log
```

### **VÃ©rification hebdomadaire**

```bash
# Espace disque
df -h

# RAM utilisÃ©e
free -h

# Nombre d'annonces scrapÃ©es
wc -l /opt/scraper-centris/scraped_properties.json
```

### **Backup automatique**

Le systÃ¨me fait dÃ©jÃ  des backups automatiques de `scraped_properties.json`.

**Pour faire un backup manuel :**
```bash
cd /opt/scraper-centris
cp scraped_properties.json scraped_properties_backup_$(date +%Y%m%d).json
```

---

## ðŸ”’ SÃ‰CURITÃ‰

### **CrÃ©er un utilisateur non-root (recommandÃ©)**

```bash
# CrÃ©er un utilisateur
adduser scraper
usermod -aG sudo scraper

# Changer le propriÃ©taire des fichiers
chown -R scraper:scraper /opt/scraper-centris

# Modifier le service
nano /etc/systemd/system/scraper-centris.service
# Changez: User=root -> User=scraper
systemctl daemon-reload
systemctl restart scraper-centris
```

### **Configurer le firewall**

```bash
# Installer UFW
apt install ufw

# Autoriser SSH
ufw allow ssh

# Activer
ufw enable

# VÃ©rifier
ufw status
```

---

## âœ… CHECKLIST POST-DÃ‰PLOIEMENT

- [ ] Le service dÃ©marre sans erreur
- [ ] Les logs montrent l'activitÃ© du scraper
- [ ] Le test API fonctionne
- [ ] scraped_properties.json se remplit
- [ ] Les fichiers property_*.json sont crÃ©Ã©s
- [ ] Le monitoring DigitalOcean montre de l'activitÃ©
- [ ] Vous recevez les donnÃ©es dans votre API

---

## ðŸ“ž SUPPORT

**En cas de problÃ¨me :**

1. VÃ©rifiez les logs : `tail -f /var/log/scraper-centris.log`
2. Testez manuellement : `python3.12 scraper_production.py`
3. VÃ©rifiez la config : `cat config_api.py`

**Commandes de diagnostic :**
```bash
# Info systÃ¨me
uname -a
python3.12 --version
google-chrome --version
chromedriver --version

# Processus en cours
ps aux | grep python

# Connexions rÃ©seau
netstat -tlnp | grep python
```

---

## ðŸŽ‰ FÃ‰LICITATIONS !

Votre scraper Centris est maintenant **en production** ! ðŸš€

Il va tourner 24/7 et scraper automatiquement les nouvelles annonces !
